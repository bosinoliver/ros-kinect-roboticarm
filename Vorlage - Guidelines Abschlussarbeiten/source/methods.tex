\chapter{Methodik}
\index{Methodik} 
In diesem Kapitel wird die verwendete Soft- und Hardware beschrieben und es wird erklärt wofür diese eingesetzt wurde. Zusätzlich wird der Deep Learning Gesamtprozess beschrieben.

\section{Verwendete Software und Hardware}


\subsection{Software}

\subsubsection{Tensorflow}
Tensorflow ist ein Framework, welches Funktionen zum erstellen von Modellen, für das maschinelle Lernen, bereitstellt. Das Framework wird als Open Source Projekt, unter der Federführung von Google, weiterentwickelt. Die Tensoren, welche typisierte multidimensionale Arrays darstellen, bilden das Hauptkonzept von Tensorflow. Diese Tensoren durchlaufen Datenflussgraphen, welche aus Knoten bestehen. Durch die Knoten werden numerische Operationen abgebildet. Die Ausführung dieser Operationen, kann mit Tensorflow auf Grafikarten ausgelagert werden. 
In dieser Arbeit wurde mit Tensorflow, in der Version 2.2, gearbeitet. Tensorflow wurde hier als Backend für Keras genutzt.

\subsubsection{Keras}
Keras ist eine High-Level-API, welche verschiedene Backends unterstützt. In dem Keras eine weitere Abstraktionsebene schafft, wird das erstellen, trainieren und evaluieren von neuronalen Netzen vereinfacht. Durch die Unterstützung von verschiedenen Backends, muss der Keras-Code nur einmal geschrieben werden und kann dann mit den verschiedenen Backends verwendet werden. 
In dieser Arbeit wurde mit Keras, in der Version 2.4, gearbeitet. Keras wurde genutzt, um Modelle zu erstellen, zu trainieren und zu testen. 

\subsubsection{OpenCV}
OpenCV ist eine Open Source Bibliothek, welche Funktionen für die Bildverarbeitung und für das maschinelle Sehen bereitstellt. In dieser Arbeit wurde mit OpenCV, in der Version 4.3.0, gearbeitet. Es wurde die Funktionen zur Gesichtserkennung sowie die Funktionen für die Verarbeitung von Videos und Bildern verwendet.

\subsubsection{Eulerian Video Magnification}
Das Paket ''Eulerian Video Magnification'' ist in Matlab-Code geschrieben und bietet Funktionen zur Anwendung des gleichnamigen Verfahrens an. In dieser Arbeit wurde mit der Version 1.1 gearbeitet. Unter Verwendung des Matlab-Kernels für Python, wurden die Funktionen des Paketes in Python verfügbar gemacht. 



\subsection{Hardware}
Für die Programmierarbeiten wurde ein Notebook, mit Ubuntu 18.04 als Betriebssystem, verwendet. Das Notebook hat einen Intel Core i7 Prozessor, 16 Gigabyte Arbeitsspeicher und eine Nvidia GTX 1050 Ti Grafikkarte mit 4 Gigabyte Grafikspeicher.

\subsubsection{Trainingssystem}
Als Trainingssystem, für neuronale Netze, wurde ein Nvidia DGX-1 System verwendet. Dieses System verfügt über sechzehn V100 Grafikkarten, mit jeweils 32 Gigabyte Grafikspeicher.

\section{Vorgehen im Deep Learning Gesamtprozess}
In diesem Abschnitt wird der Deep Learning Gesamtprozess beschrieben. Es wird nur auf die verwendeten Daten und auf die Vorgehensweise eingegangen.

\subsection{Datensatz}
Als Datensatz, für das Training und die Validierung, wurde der Celeb-DF(V2) Datensatz verwendet. Dieser Datensatz besteht aus 590 realen Videos und 5639 DeepFake-Videos. Die Videos sind im Durchschnitt 13 Sekunden lang und haben eine Bildwiederholungsrate von 30 Bilder/s. Die realen Videos zeigen 59 verschiedene Bekanntheiten. Die gezeigte Personengruppe besteht aus 56.8 \% Männern und 43.2 \% Frauen. Hiervon sind 8.5 \% älter als 60, 30.5 \% 50-60, 26.6 \% in den Vierzigern und 28.0 \% 30-40. Die ethnischen Gruppen sind 5.1 \% Asiaten, 6.8 \% Afro-Amerikaner und 88.1 \% weiße Amerikaner. Die Größe der Gesichter in Pixeln variert in den Videos. Die DeepFake Videos wurden durch das Tauschen der Gesichter für jedes Paar der 59 Personen generiert. Für die Generierung der DeepFake Videos wurde ein Autoencoder verwendet. Alle Videos liegen im MPEG4.0 Format vor.% ( Todo Zitat@misc{li2020celebdf,
 %     title={Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics}, 
%      author={Yuezun Li and Xin Yang and Pu Sun and Honggang Qi and Siwei Lyu},
 %     year={2020},
%      eprint={1909.12962},
%      archivePrefix={arXiv},
%      primaryClass={cs.CR}
%})


\subsection{Testdaten}
Da die Anzahl an realen Videos, für das Training eines neuronalen Netzes, relativ gering ist, wurden die gleichen Videos zum validieren und zum testen verwendet. Diese Videos wurden, vor dem Beginn des Trainings, von den Trainingsdaten separiert und es wurde nie mit diesen Videos trainiert. Dieser Datensatz enthält 177 reale Videos und 340 DeepFake Videos. Durch die Selektion von Netzen, welche während des Trainings gute Ergebnisse auf den Validierungsdaten erreichten, könnten die Ergebnisse der Tests zum positiven verfälscht sein. Um die erreichte Fähigkeit der Netze zum generalisieren besser einschätzen zu können, wurde ein zweiter Testdatensatz erstellt. Dieser zweite Testdatensatz besteht ebenfalls aus 177 realen Videos und 340 DeepFake Videos. Die Videos des zweiten Testdatensatzes stammen aus dem ''FaceForensics++'' Datensatz. Es wurden Videos ausgewählt, welche, wie die Videos aus dem Hauptdatensatz, mit dem FaceSwap Verfahren erstellt wurden.

\subsection{Vorverarbeitung der Daten}
Vor dem Training der Modelle wurden die Videos vorverarbeitet. Um so wenig störenden Hintergrund wie möglich im Bild zu haben, wurde eine Gesichtserkennung auf die Videos angewendet und der Hintergrund herausgeschnitten. Anschließend wurde die ''Eulerian Video Magnification'' auf alle Videos angewendet. Hier wurde gezielt das Frequenzband verstärkt, in welches auch die Herzfrequenz fällt. Die folgenden Schritte wurden auf die zugeschnittenen Videos sowie auf die zugeschnittenen und verstärkten Videos angewendet. Dies bietet die möglichkeit die Wirkung der ''Eulerian Video Magnification'' auf die Ergebnisse beim Training und Test darzustellen. Da manche Videos zu lang waren und auch in der Länge zu stark variierten, wurden die Videos in mehrere kurze Videos zugeschnitten. Um die Videos mit einem 1D-CNN verarbeiten zu können, wurden die Videos in eine zweidimensionale Representation umgewandelt. Hierzu wurde aus jedem Einzelbild eine horizontale Pixelreihe, auf der Höhe von 60 \% der Gesamthöhe des Einzelbildes, von 150 Pixeln herausgeschnitten. Auf dieser Höhe sollten sich die Nase und die Wangen befinden, welche meist stärker durchblutet sind als andere Bereiche im Gesicht. Um eine zweidimensionale Representation zu erhalten, wurden diese Pixelreihen, in der zeitlichen Abfolge im Video, untereinander angeordnet. Diese Representation des Videos wurde als Bild im JPEG Format gespeichert. In Abbildung \ref{fig:Beispiel erstellen von 2D-Representation} ist an einem Beispiel das Vorgehen und das Ergebnis visualisiert. Um das Signal, welches von der ''Eulerian Video Magnification'' verstärkt wurde, weiter zu isolieren, sind Differenzbilder und Differenzvideos erstellt worden. Dazu wurden die Pixelwerte der Originalbilder von den Pixelwerten der verstärkten Bilder subtrahiert und der Betrag jeweils gebildet. Folgende Datensätze für das Training standen, nach der vorangegangenen Vorverarbeitung, zu Verfügung:

\begin{enumerate} 
	\item Die Videos mit reduziertem Hintergrund im Original und verstärkt. 
	\item Die Differenzvideos
	\item Die zweidimensionalen Representationen, der Original Videos und der verstärkten Videos, als Bild.
	\item Die zweidimensionalen Representationen als Differenzbilder
\end{enumerate}

\begin{figure}
	\centering
		\includegraphics[width=0.3\textwidth]{images/methods/slicing.png}
	\caption{Beispiel erstellen von 2D-Representation}
	\label{fig:Beispiel erstellen von 2D-Representation}
\end{figure}


\subsection{Training und Test}
Die verwendeten Netze wurden jeweils mit jedem vorverarbeiteten Trainingsdatensatz trainiert. Die Bereitstellung der Daten und das Training an sich wurden mit Keras realisiert. Die Ausgaben während des Trainings wurden in Textdateien geschrieben, um den Trainingsverlauf, auch bei Abbruch durch Fehler, nachvollziehen zu können. Nach der Beedigung jedes Trainings wurden die Verläufe des Fehlers und der AUC(Area Under the ROC Curve) in einem Diagram gespeichert. Der AUC Wert steht für die Fläche unter der ROC Kurve (receiver operating characteristic curve). Die ROC Kurve zeigt das Verhältnis zwischen der Spezifität und der Sensitivität des Netzes, für verschiedene Klassifikationsschwellen. Der AUC Wert eignet sich für die Bewertung von binären Klassifikatoren wesentlich besser als die Genauigkeit, da die AUC weniger durch die Menge der verwendeten Daten und das Klassenverhältnis beeinflusst wird. Im nächsten Schritt wurde der Fehler und die AUC des Netzes für die Testdatensätze ermittelt. 


\begin{figure}
	\centering
		\includegraphics[width=0.6\textwidth]{images/methods/AUC.png}
	\caption{Beispiel für eine ROC Kurve und die AUC}
	\label{fig:Beispiel für eine ROC Kurve und die AUC}
\end{figure}