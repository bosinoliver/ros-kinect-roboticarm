%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Vorlage für Abschlussarbeiten                                     %%
%%-------------------------------------------------------------------%%
%% Datei:        results.tex                                         %%
%% Beschreibung: Ergebnissteil der Arbeit der die erstellte Hard-    %%
%%               und Software beschreibt.                            %%
%% Autor: 			 Stefan Herrmann                                     %%
%% Datum:        04.12.2012                                          %%
%% Version:      1.0.1                                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation des Gestenerkennungssystems mit ROS}
In diesem Kapitel wird die gesamte Implementation, des Gestenerkennungssystems, beschrieben. Das Kapitel ist so gegliedert, sodass die Abfolge der Schritte im Text auch die einzuhaltende Implementationsreihenfolge darstellt. Bei nicht einhalten der Implementationsreihenfolge sind Konflikte zwischen Paketen nicht auszuschließen.

\section{Vorbereitung}
Die Vorbereitenden Maßnahmen werden in diesem Abschnitt beschrieben. 

\subsection{Catkin Workspace erstellen}
Um veränderte und eigene Pakete einfach zu compilieren und zu bauen, ist es notwendig einen Catkin-Workspace zu erstellen. Die folgenden Befehle sind in einem Terminal einzugeben, um einen Catkin-Workspace zu erstellen und initial zu bauen:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ mkdir -p ~/catkin_ws/src|\\
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ~/catkin_ws/|\\
\lstinline[basicstyle=\ttfamily\color{black}]|$ catkin_make|\\ \hline
		
\end{tabularx}
\\
\\
Nachdem der Catkin-Workspace erstellt und gebaut wurde, sollte die Ordnerstruktur wie in Abbildung \ref{fig:Ordnerstruktur Catkin-Workspace} dargestellt aussehen. Im Ordner ''src'' wurde eine Datei mit dem Namen ''CMakeLists.txt'' generiert. Im Ordner ''devel'' wurden verschiedene Setup-Dateien generiert. Um den Pfad des Catkin-Workspace in die Umgebungsvariablen aufzunehmen, muss der folgende Befehl ausgeführt werden:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ source devel/setup.bash|\\ \hline

\end{tabularx}
\\
\\
Weitergehende Informationen zu ''Catkin'' sind im ROS Wiki'\footnote[8]{http://wiki.ros.org/catkin} zu finden.

\begin{figure}
	\centering
		\includegraphics[width=0.6\textwidth]{images/results/CatkinWorkspace.png}
	\caption{Ordnerstruktur Catkin-Workspace}
	\label{fig:Ordnerstruktur Catkin-Workspace}
\end{figure}

\subsection{Abhängigkeiten}
Die Komponenten des Gestenerkennungssystems haben Abhängigkeiten zu anderen Paketen. Diese Abhängigkeiten müssen vor der Implementation, des Gestenerkennungssystems, erfüllt sein. Um diese Abhängigkeiten zu erfüllen, werden mit den folgenden Befehlen die benötigten Pakete installiert:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
 \lstinline[basicstyle=\ttfamily\color{black}]|$ sudo apt-get install git build-essential python libusb-1.0-0-dev freeglut3-dev openjdk-8-jdk|\\
 \lstinline[basicstyle=\ttfamily\color{black}]|$ apt-get install doxygen graphviz mono-complete|\\ \hline

\end{tabularx}
\\
\\

\section{Installation der Komponenten}
In diesem Abschnitt werden die Komponenten des Gestenerkennungssystems installiert. Um die Installation von ''OpenNI'' und der dazu gehörigen Module von den Paketen getrennt zu halten, wird ein neuer Ordner erstellt. Der folgende Befehl erstellt den neuen Ordner:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
 \lstinline[basicstyle=\ttfamily\color{black}]|$ mkdir -p ~/kinect|\\ \hline

\end{tabularx}
\\
\\


\subsection{OpenNI installieren}
Um ''OpenNI'' zu installieren wird erst ein Repository, in den Ordner ''kinect'', geklont und ''OpenNI'' aus dieser Kopie heraus installiert. Die folgenden Befehle führen dies aus:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ~/kinect|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ git clone https://github.com/OpenNI/OpenNI.git|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd OpenNI|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ git checkout Unstable-1.5.4.0|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd Platform/Linux/CreateRedist|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ chmod +x RedistMaker|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ ./RedistMaker|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ../Redist/OpenNI-Bin-Dev-Linux-x64-v1.5.4.0|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ sudo ./install.sh|\\ \hline

\end{tabularx}
\\
\\
Das Modul ''SensorKinect'' für ''OpenNI'' wird durch analoges Vorgehen installiert. Die folgenden Befehle sind auszuführen:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ~/kinect|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ git clone https://github.com/avin2/SensorKinect|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd SensorKinect|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd Platform/Linux/CreateRedist|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ chmod +x RedistMaker|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ ./RedistMaker|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ../Redist/Sensor-Bin-Linux-x64-v5.1.2.1|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ chmod +x install.sh|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ sudo ./install.sh|\\ \hline

\end{tabularx}
\\
\\
\subsection{Ros-Kinetic-OpenNI installieren}
In diesem Schritt wird die Sammlung von Paketen, die für den Zugriff auf die Daten der ''Kinect'' unter ROS notwendig sind, installiert. Um die Installation durchzuführen ist der folgende Befehl auszuführen:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ sudo apt-get install ros-kinetic-openni*|\\ \hline

\end{tabularx}
\\
\\

\subsection{NITE installieren}
Die Middleware ''NITE'' wird mit folgenden Befehlen installiert und bei ''OpenNI'' als Modul registriert:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ~/kinect|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ git clone https://github.com/arnaud-ramey/NITE-Bin-Dev-Linux-v1.5.2.23|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd NITE-Bin-Dev-Linux-v1.5.2.23/x64|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ chmod +x install.sh|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ sudo ./install.sh|\\ \hline

\end{tabularx}
\\
\\

\subsection{OpenNI Tracker installieren}
Das Paket ''OpenNI Tracker'' wird von einem Repository in den Catkin-Workspace kopiert. Anschließend wird der Catkin-Workspace erneut gebaut und das Paket ''OpenNI Tracker'' installiert. Diese Schritte werden mit folgenden Befehlen ausgeführt:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ~/catkin_ws/src|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ git clone https://github.com/ros-drivers/openni_tracker.git|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ~/catkin_ws|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ catkin_make|\\ 
\lstinline[basicstyle=\ttfamily\color{black}]|$ catkin_make install|\\ \hline

\end{tabularx}
\\
\\


\section{Start und Test des Gestenerkennungssystems}
In diesem Abschnitt wird beschrieben wie die Komponenten des Gestenerkennungssystems gestartet werden und das Gestenerkennungssystem getestet wird. Bevor die Komponenten gestartet werden können, muss die''Kinect'', über einen freien USB 3.X Port, mit dem Rechner verbunden werden. Zusätzlich muss sichergestellt werden, dass die virtuelle Maschine Zugriff auf die ''Kinect'' hat. Dies ist der Fall, wenn die virtuelle Maschine die ''Kinect'' als angeschlossene Hardware erkennt.

\subsection{Starten der Komponenten}
In einem Terminal wird der folgende Befehl ausgeführt, um die Knoten zu starten, welche die Daten der ''Kinect'' in ROS veröffentlichen:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ roslaunch openni_launch openni.launch camera:=openni|\\ \hline

\end{tabularx}
\\
\\
Der Knoten ''OpenNI Tracker'', der die Daten des erkannten Skeletts in ROS veröffentlicht, wird in einem zweiten Terminal mit folgenden Befehl gestartet:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ roslaunch openni_tracker openni_tracker|\\ \hline

\end{tabularx}
\\
\\
Wenn eine Person, nach Start des Knotens ''OpenNI Tracker'', vor die ''kinect'' tritt, dann gibt der Knoten den Text ''New User 1'' im Terminal aus. Die Person muss nun die ''Psi-Pose'', wie in Abbildung \ref{fig:PsiPose} dargestellt, einnehmen, um die Kalibrierung des Skeletts zu starten. Die optimale Entfernung für die Erkennung des Skeletts sind 2,5 Meter. Das die Pose korrekt eingenommen wurde, wird mit dem Text ''Pose Psi detected for user 1'' quittiert. Der Knoten startet nun die Kalibrierung und gibt dies mit dem Text ''Calibration started for user 1'' an. Wenn die Kalibrierung abgeschlossen wurde, gibt der Knoten den Text s''Calibration complete, start tracking user 1'' aus und beginnt die Positionsdaten des Skeletts zu veröffentlichen. 

\begin{figure}
	\centering
		\includegraphics[width=0.2\textwidth]{images/results/PsiPose.png}
	\caption{Psi Pose}
	\label{fig:PsiPose}
\end{figure}

\subsection{Mit Rviz visualisieren und überprüfen}
Um die Funktion des Gestenerkennungssystems zu überprüfen, wird hier mit dem Tool ''Rviz'' gearbeitet. Im folgenden Abschnitt wird beschrieben wie eine ''Point Cloud'' und das, von ''OpenNI Tracker'' erkannte, Skelett mit TF's in ''Rviz'' visualisiert wird.\\
''Rviz'' wird mit folgenden Befehl gestartet:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ rosrun rviz rviz|\\ \hline

\end{tabularx}
\\
\\
In Abbildung \ref{fig:FixedFrame} ist der Bereich Displays, des Fensters von ''Rviz'', abgebildet. Im Bereich ''Displays'' wird als ''Fixed Frame'' die Auswahlmöglichkeit ''openni\_link'' eingestellt. Im Fenster, unter dem Bereich ''Displays'', wird der Button ''Add'' angeklickt. In Abbildung \ref{fig:AddDepthCloud} ist das sich öffnende Fenster abgebildet. In diesem Fenster wird der Reiter ''By topic'' ausgewählt. Unter diesem Reiter wird in der Liste, oben im Fenster, unter ''openni/depth/image'' die Zeile ''Depthcloud'' angeklickt und anschließend mit einem Klick auf ''OK'' bestätigt. Im Bereich ''Displays'' wurde eine Zeile mit dem Text ''DepthCloud'' hinzugefügt. Durch einen Klick auf das Dreieck, links neben dem Text, wird die Zeile, wie in Abbildung \ref{fig:ColorImageTopic} abgebildet, erweitert. Hier wird als ''Color Image Topic'' die Auswahlmöglichkeit ''openni/rgb/image\_color'' ausgwählt, um der ''DepthCloud'' Farbinformationen hinzuzufügen. Nachdem die ''DepthCloud'' eingerichtet wurde, wird nun erneut auf den Button ''Add'' geklickt. Wie in Abbildung \ref{fig:AddTF} abgebildet wird, unter dem Reiter ''By display type'', die Zeile ''TF'' markiert und mit einem Klick auf ''OK'' bestätigt. Im Bereich ''Displays'' wurde, wie in Abbildung \ref{fig:DisplaysFinal} zu sehen, eine Zeile mit dem Text ''TF'' hinzugefügt. Wenn, wie in Abbildung \ref{fig:DepthCloudAndSkelett} abgebildet, die ''DepthCloud'' richtig dargestellt wird und das erkannte Skelett korrekt positioniert ist, ist die korrekte Funktion des Gestenerkennungssystems gegeben.

\begin{figure}
	\centering
		\includegraphics[width=0.4\textwidth]{images/results/rviz1.png}
	\caption{Rviz - Fixed Frame Auswahl}
	\label{fig:FixedFrame}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=0.5\textwidth]{images/results/rviz2.png}
	\caption{Rviz - DepthCloud hinzufügen}
	\label{fig:AddDepthCloud}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=0.4\textwidth]{images/results/rviz3.png}
	\caption{Rviz - Color Image Topic Auswahl}
	\label{fig:ColorImageTopic}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=0.5\textwidth]{images/results/rviz6.png}
	\caption{Rviz - TF hinzufügen}
	\label{fig:AddTF}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=0.4\textwidth]{images/results/rviz4.png}
	\caption{Rviz - Displays Endergebnis}
	\label{fig:DisplaysFinal}
\end{figure}

\begin{figure}
	\centering
		\includegraphics[width=0.9\textwidth]{images/results/rviz5cut.png}
	\caption{Rviz - Visualisierung von DepthCloud und Skelett}
	\label{fig:DepthCloudAndSkelett}
\end{figure}

\chapter{Implementation von Gestensteuerungen basierend auf dem Gestenerkennungssystem}
In diesen Kapitel wird auf die Entwicklung der Pakete ''turtlesim\_gesture\_control'' und ''roboticarm\_gesture\_control'' eingegangen. Innerhalb dieser zwei Pakete wurden Gestensteuerungen implementiert. Für die Implementation der Gestensteuerungen wurde das in dieser Arbeit entwickelte Gestenerkennungssystem verwendet.


\section{Vorbereitung}
In diesem Abschnitt wird auf die Vorbereitungen eingegangen, welche für die Entwicklung der zwei Pakete nötig waren.

\subsection{Installation MoveIt!}
Für die Entwicklung des Paketes ''roboticarm\_gesture\_control'' wurde das Framework ''MoveIt!'' verwendet. Um ''MoveIt!'' zu installieren, muss der folgende Befehl in einem Terminal ausgeführt werden:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ sudo apt-get install ros-kinetic-moveit|\\ \hline

\end{tabularx}
\\
\\

\subsection{Einrichtung der Pakete rob\_arm\_small und  rob\_arm\_small\_hw\_interface}
Zur Ansteuerung des Roboterarmes ''rob\_arm\_small'' wurden die Pakete ''rob\_arm\_small'' und ''rob\_arm\_small\_hw\_interface'' verwendet. Diese Pakete wurden in einer vorangegangenen Bachelorarbeit\cite{WALDNER2018} von Christian Waldner entwickelt.\\
Um die Pakete getrennt vom Gestensteuerungssystem zu halten, wurde ein weiterer Catkin-Workspace mit dem Namen ''ROS\_ws'' erstellt. Folgende Befehle führen dies aus:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ mkdir -p ~/ROS_ws/src|\\
\lstinline[basicstyle=\ttfamily\color{black}]|$ cd ~/ROS_ws/|\\
\lstinline[basicstyle=\ttfamily\color{black}]|$ catkin_make|\\ \hline
		
\end{tabularx}
\\
\\

In diesen Catkin-Workspace werden die beiden Pakete kopiert. Der Catkin-Workspace wird mit folgenden Befehlen erneut gebaut und in den Umgebungsvariablen aufgenommen:\\
\\
\begin{tabularx}{\textwidth}{|X|}
\hline
\lstinline[basicstyle=\ttfamily\color{black}]|$ catkin_make|\\
\lstinline[basicstyle=\ttfamily\color{black}]|$ source devel/setup.bash|\\ \hline

\end{tabularx}
\\
\\

\subsection{Erstellung eines ROS Paketes}



\section{turtlesim\_gesture\_control}


\section{roboticarm\_gesture\_control}
















\section{Formeln}
\index{Formeln}
Formeln können zum einen direkt inline im Text oder abgesetzt eingefügt werden. Wird sich für eine abgesetzte Formel entschiede, dann sind diese zu nummerieren, um sie im Text referenzieren zu können. Alle etwas komplizierteren Formeln sollten abgesetzt werden, um das Schriftbild übersichtlich zu halten. Einfache Zusammenhänge, wie beispielsweise $x_i = \cos{(i - 1)}$, stören das Schriftbild aber nicht weiter. Als Beispiel für eine abgesetzte Formel sei auf Formel \ref{eq:Formel} oder \ref{eq:Formel2} verwiesen.

\begin{align}
f(x_{k,i}, x_{k+1,i})=
\begin{cases}
1	& \text{falls} \hspace{1cm} (-x_{k,i} \cdot x_{k+1,i}) > 0 \wedge |x_{k,i} - x_{k+1,i}| > \text{Schwellwert}\\
0 & \text{sonst}
\end{cases}
\label{eq:Formel}
\end{align}

\begin{align}
A = 
\left(
\begin{array}{ccc}
a   &   b    &    c \\
d   &   e    &    f \\
g   &   h    &    i 
\end{array}
\right)
\label{eq:Formel2}
\end{align}
