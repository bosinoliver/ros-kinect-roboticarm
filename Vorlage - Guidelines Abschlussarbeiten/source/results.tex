%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Vorlage für Abschlussarbeiten                                     %%
%%-------------------------------------------------------------------%%
%% Datei:        results.tex                                         %%
%% Beschreibung: Ergebnissteil der Arbeit der die erstellte Hard-    %%
%%               und Software beschreibt.                            %%
%% Autor: 			 Stefan Herrmann                                     %%
%% Datum:        04.12.2012                                          %%
%% Version:      1.0.1                                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation}
Die Implementation von Funktionen, für die Vorverarbeitung der Datensätze, und die Implementation der verwendeten neuronalen Netze wird in diesem Kapitel beschrieben.

\section{Erstellen der Datensätze}
In diesem Abschnitt wird der Ablauf der Generierung der Datensätze näher beschrieben und die dazu implementierten Funktion erklärt. Es werden nur die essentielen Funktion beschrieben. Genutzte Hilfsfunktionen liegen im Quellcode, der im Anhang dieser Arbeit zu finden ist, vor.

\subsection{Entfernen des Hintergrundes}
Um so viel Hintergrund wie möglich zu entfernen und dennoch keine Teile des Gesichtes zu entfernen, wurden mehrere Schritte durchgeführt. Zusätzlich war es ein Ziel ein Video zu erhalten, bei welchem das Gesicht möglichst statisch und zentral im Bild gehalten wird, da es sich herausgestellt hat, dass dies sich positiv auf das Resultat der ''Eulerian Video Magnification'' auswirkt. Im ersten Schritt wird das Video mit OpenCV geöffnet. Die Auflösung und die Anzahl der Einzelbilder wird ermittelt. Nun wird jedes Einzelbild nacheinander eingelesen. Es wird eine Gesichtserkennung mit OpenCV auf jedes Einzelbild angwendet. Die Gesichtserkennungsfunktion gibt die Koordinaten eines Rahmens zurück, welcher das gefundene Gesicht umrahmt. Diese Koordinaten werden in Arrays gespeichert und die eingelesen Einzelbilder unbearbeitet in einen Videotensor geschrieben. Von den Videotensoren werden die ersten und die letzten 20 Einzelbilder und die dazugehörigen Gesichtskoordinaten aus den Arrays enfernt, da dort oft Einblendeffekte vorhanden sind oder die ersten Einzelbilder den Abschluss eines Kamerawechsels zeigen. Effekte und Vorgänge dieser Art können, dass Resultat der ''Eulerian Video Magnification'', negativ beeinflussen. Da die Bilder, nach entfernen des Hintergrundes, alle die gleiche Größe haben müssen, wird aus den Gesichtskoordinaten die durchschnittliche Rahmengröße berechnet. Um ein Wackeln des Rahmens zu verhindern, werden die Koordinaten mit einer Glättungsfunktion gefiltert. Hierdurch folgt der Rahmen dynamisch dem Gesicht im Video, ohne ein Zittern des Bildes zu erzeugen. Wenn für jedes Einzelbild der Rahmen berechnet wurde, dann wird mit OpenCV der Teil des Originalbildes, welcher von dem Rahmen umschlossen wird, in eine Videodatei geschrieben und das Video gespeichert. 

\subsection{Anwenden der ''Eulerian Video Magnification''}
Um die ''Eulerian Video Magnification'' in Python auf ein Video anwenden zu können, wird der Matlab-Kernel für Python gestartet. Durch den Aufruf der Matlab-Funktion ''amplify\_spatial\_Gdown\_temporal\_ideal'' des Paketes ''Eulerian Video Magnification'' wird das Video geöffnet, mit den gewählten Fuktionsparametern verarbeitet und das Zielvideo abgespeichert.
Folgende Funktionsparameter haben sich, für den genutzten Datensatz, in Experimenten als effektiv ergeben:

\begin{enumerate} 
	\item Verstärkungsfaktor = 12.0
	\item Anzahl an Bildpyramidenstufen = 3
	\item untere Grenzfrequenz = 0.75 Hertz
	\item obere Grenzfrequenz = 1.67 Hertz
	\item Dämpfungsfaktor für Chrominanz = 0.7
\end{enumerate}

\subsection{Erstellen der 2D-Representationen}
Um eine 2D-Representation eines Videos zu erhalten, wird jedes Einzelbild geladen und die Pixelreihe auf 60\% der Höhe des Einzelbildes herausgeschnitten. Die Pixelreihen werden in einen Tensor untereinander angeordnet. Der Tensor wird mit OpenCV in ein Bild im JPEG Format umgwandelt und abgespeichert.

\subsection{Kürzen der Videos}
Die Videos werden mit OpenCV geöffnet. Es werden immer jeweils 30 Einzelbilder in einem Tensor abgelegt und mit OpenCV daraus gekürzte Videos erstellt und im MPEG4 Format gespeichert.

\subsection{Erstellen der Differenzvideos}
Das Originalvideo und das verstärkte Video werden mit OpenCV geöffnet. Jedes Einzelbild des Originalvideos wird von dem jeweils korrespondierenden Bild im verstärkten Video subtrahiert und von dem Ergebis der Betrag gebildet. Mit OpenCV wird aus den berechneten Differenzbildern ein Video erstellt und im MPEG4 Format gespeichert.

\subsection{Erstellen der Differenzbilder}
Die 2D-Representation des Originalvideos und die des verstärkten Videos wird geöffnet. Die 2D-Representation des Originalvideos wird von der des verstärkten Videos subtrahiert und von dem Ergebnis der Betrag gebildet. Das Ergebnis wird mit OpenCV als Bild im JPEG Format gespeichert.

\section{Erstellen der Modelle}
In diesem Abschnitt werden die verwendeten neuronalen Netze und deren Implementation beschrieben.

\subsection{1D-OS-CNN}
Mit der Architektur des 1D-OS-CNN, wurde die Auswahl der Kernelgröße in den Lernprozess integriert. Dies bedeutet, dass ein Hyperparameter weniger gewählt werden muss, da die Suche nach der optimalen Kernelgröße ein Teil des Lernprozesses ist. 

\subsection{Xception}

\subsection{3D-CNN}

\chapter{Experimente und Evaluation}

\section{Training}

\subsection{1D-OSCNN}

\subsubsection{Training 1D-OSCNN \#1}

\subsubsection{Training 1D-OSCNN \#2}

\subsubsection{Training 1D-OSCNN \#3}

\subsection{Xception}

\subsubsection{Training Xception \#1}

\subsubsection{Training Xception \#2}

\subsubsection{Training 3D-CNN \#1}

\subsection{3D-CNN}

\subsubsection{Training 3D-CNN \#1}

\subsubsection{Training 3D-CNN \#1}

\subsubsection{Training 3D-CNN \#1}

\section{Test}

\subsection{1D-OSCNN}

\subsubsection{Test 1D-OSCNN \#1}

\subsubsection{Test 1D-OSCNN \#2}

\subsubsection{Test 1D-OSCNN \#3}

\subsection{Xception}

\subsubsection{Test Xception \#1}

\subsubsection{Test Xception \#2}

\subsubsection{Test 3D-CNN \#1}

\subsection{3D-CNN}

\subsubsection{Test 3D-CNN \#1}

\subsubsection{Test 3D-CNN \#1}

\subsubsection{Test 3D-CNN \#1}

\section{Einordnung der Ergebnisse}

\section{Benchmark}

