%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Vorlage für Abschlussarbeiten                                     %%
%%-------------------------------------------------------------------%%
%% Datei:        basics.tex                                         %%
%% Beschreibung: Grundlagenteil welcher verwendete Hard-   %%
%%               und Software näher beschreibt.                            %%
%% Autor: 			 Stefan Herrmann                                     %%
%% Datum:        04.12.2012                                          %%
%% Version:      1.0.1                                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Grundlagen}
\index{Grundlagen} %% Eintrag im Stichwortverzeichnis
In diesem Kapitel wird auf die Grundlagen, zu verwendeten Verfahren sowie zu verwendeten neuronalen Netzen, eingegangen. Dies soll die notwendigen Informationen, zur gesamten Arbeit und speziell für die Ausführungen im dritten und vierten Kapitel, bereitstellen. Hierbei werden nur die, für das Verständnis der Arbeit, wesentlichen Komponenten beschrieben.

\section{Eulerian Video Magnification}
\index{Eulerian Video Magnification}
Die menschliche Fähigkeit, Änderungen in der Umgebung visuell wahrzunehmen, hat eine beschränkte räumlich-zeitliche Empfindlichkeit. Dies hat als Konsequenz, dass Änderungen die Außerhalb dieses Empfindlichkeitsbereiches liegen, nicht von Menschen wahrgenommen werden können. Viele dieser subtilen Änderungen, die außerhalb der menschlichen Wahrnehmung liegen, beinhalten jedoch Informationen die von Interesse sein können. Zum Beispiel ändert sich, durch die zeitlich unterschiedliche Durchblutung, die Hautfarbe im Gesicht eines Menschen über die Zeit. Diese nicht wahrnehmbare Änderung, wenn sichtbar gemacht, kann zum Beispiel für die visuelle Messung der Pulsfrequenz einer Person genutzt werden.\cite{Philips,Poh:10,Verkruysse:08}
Um diese Informationen in Videos sichtbar zu machen, gibt es mehrere Ansätze. Neben der ''Lagrangian Motion Magnification'', welche mit optischem Fluss arbeitet, gibt es die ''Eulerian Video Magnification''. Letztere Methode kombiniert räumliche und zeitliche Verarbeitung, um die subtilen zeitlichen Änderungen in einem Video hervorzuheben. Im Gegensatz zur Verwendung von optischem Fluss zur Schätzung von Änderungen, ist dieser Ansatz nicht besonders rechenintensiv und somit auch für Echtzeitanwendungen geeignet.(todo Zitat eulerianmagnificationpaper)

\subsection{Ablauf der Eulerian Video Magnification}
\index{Ablauf der Eulerian Video Magnification}
Im ersten Schritt, wird das Video in unterschiedliche Ortsfrequenzbänder zerlegt. Diese Zerlegung wird durch den Aufbau von Bildpyramiden erreicht. Als nächstes werden diese Ortsfrequenzbänder, durch anwenden der Fast Fourier Transformation, in den Zeitbereich überführt und dort verarbeitet. Hierzu wird angenommen, dass die Zeitfolge mit dem Wert eines Pixels, in einem Frequenzband, korrespondiert und es wird ein Bandpass angewendet. Hiermit sollen die Frequenzbänder, die von Interesse sind, extrahiert werden. Das extrahierte Signal wird mit einem Verstärkungsfaktor multipliziert und auf das Ausgangssignal addiert. Im letzten Schritt wird aus den resultierenden Bildpyramiden, dass Zielvideo rekonstruiert. Der Prozess, der ''Eulerian Video Magnification'', ist in Abbildung 1 visuell dargestellt.

\begin{figure}
	\centering
		\includegraphics[width=1.0\textwidth]{images/basics/ProcessEulerianVideoMagnification.png}
	\caption{Prozess ''Eulerian Video Magnification''}
	\label{fig:Prozess ''Eulerian Video Magnification''}
\end{figure}

\subsection{Bildpyramiden}
Im Bereich der Bildanalyse und Bildmanipulation ist es oft von Vorteil, wenn das Bild auf mehreren Skalen analysiert oder manipuliert werden kann. Durch die Multiskalenanalyse wird eine Invarianz bezüglich der Größe erreicht und die Untersuchung von verschiedenen Strukturen im Bild erleichtert. Die Verwendung von Skalen bei der Bildanalyse führt eine weitere Dimension in die Bilder ein und verursacht dadurch einen signifikanten Anstieg des Speicherverbrauches, sowie einen signifikanten Anstieg des Rechenaufwandes. Das Konzept der Bildpyramiden wurde eingeführt, um genau dieses Problem, des Anstieges von Speicherverbrauch und Rechenaufwand, zu reduzieren. Die Idee hinter den Bildpyramiden ist einfach. Um feinere Skalen darstellen zu können, muss das Bild in der vollen Auflösung vorliegen. Sollen nun aber grobe Strukturen im Bild analysiert werden, dann reicht für diese Analyse eine niedriegere Auflösung aus. Die Bildpyramide beinhaltet also eine Folge von Bildern, wobei die Bilder von Stufe zu Stufe der Pyramide eine abnehmende Auflösung haben, dass heißt die Bilder werden kleiner. Diese Representation wird durch eine iterative Filterung und Unterabtastung erreicht. Obwohl Bildinformationen auf mehreren Skalen gespeichert werden, benötigt die Pyramide nur etwa ein Drittel mehr Speicher als das Originalbild. Neben dem geringeren Speicherbedarf werden, durch die Verwendung des selben Glättungsfilters auf allen Stufen, für die Berechnung der gesamten Pyramide nur vier Drittel der Operationen für ein zweidimensionales Bild benötigt. Nachdem die Pyramide einmal berechnet wurde, können Nachbarschaftsoperationen mit großen Skalen in den oberen Ebenen der Pyramide durchgeführt werden. Bei der ''Eulerian Video Magnification'' werden Bildpyramiden eingesetzt, um die einzelnen Frames des Videos in unterschiedliche Ortsfrequenzbänder zu zerlegen. Dies ist notwendig, da diese unterschiedlichen Ortsfrequenzbänder möglicherweise verschieden stark verstärkt werden sollen. Die Entscheidung hier verschiedene Verstärkungsfaktoren zu verwenden, kann durch unterschiedliche Signal-Rausch-Abstände, in den Ortsfrequenzbändern, begründet sein. (todo Zitat https://www.cg.tuwien.ac.at/courses/EinfVisComp/Skriptum/SS13/EVC-16%20Multiscale%20Representations.pdf, digitale Bildverabreitung Buch)

\subsubsection*{Gauß-Pyramide}
Eine Pyramide, die mit einem Gaußfilter konstruiert wurde, heißt Gaußpyramide. In der Abbildung 2 ist die Vorgehensweise, von Tiefpassfilterung und der Unterabtastung um den Faktor zwei, visuell, an einem eindimensionalen Beispiel, dargestellt.  Die Bilder, in der Gaußpyramide, wurden tiefpass-gefiltert, wobei die Grenzfrequenz von Stufe zu Stufe auf die Hälfte reduziert wurde. Hierdurch verbleiben, von Stufe zu Stufe, zunehmend grobe Strukturen in den Bildern.

\begin{figure}
	\centering
		\includegraphics[width=0.7\textwidth]{images/basics/GausspyramideProzess.png}
	\caption{Vorgehen Konstruktion einer Gauß-Pyramide}
	\label{fig:Vorgehen Konstruktion einer Gauß-Pyramide}
\end{figure}

\subsubsection*{Laplace-Pyramide}
Alternativ zu der Tiefpassfilterung bei der Gauß-Pyramide, kann eine Pyramide so konstruiert werden, sodass in ihren Stufen eine Bandpassfilterung des Originalbildes enthalten ist. Durch die Subtraktion zweier aufeinanderfolgender Bilder in einer Gauß-Pyramide, wird eine solche Bandpassfilterung realisiert. Eine Laplace-Pyramide ist so eine Pyramide. Um das Bild in der oberen Stufe von der unteren Stufe zu subtrahieren, muss dieses vorher auf die gleiche Größe expandiert werden. Im Gegensatz zu der Größenreduktion ist die Expansion rechenintensiver, da die fehlenden Informationen interpoliert werden müssen. 
In der Laplace-Pyramide werden auf den ersten Stufen feinere Kantenstrukturen betont und auf den oberen Stufen zunehmend grobe Kantenstrukturen des Originialbildes betont. Die Approximation der zweiten Ableitung des Originalbildes wird druch die Laplace-Pyramide dargestellt. Ein bedeutender Vorteil der Laplace-Pyramide ist, dass durch die rekursive Expandierung und Aufsummierung, der enthaltenen Bildserie, das Originalbild schnell wiederhergestellt werden kann. Dies entspricht einer Umkehrung des Konstruktionsablaufs. Die Laplace-Pyramide wird bei der ''Eulerian Video Magnification'' verwendet, um das Video in unterschiedliche Ortsfrequenzbänder zu zerlegen.   (todo Zitat https://www.cg.tuwien.ac.at/courses/EinfVisComp/Skriptum/SS13/EVC-16%20Multiscale%20Representations.pdf, digitale Bildverabreitung Buch)

\begin{figure}
	\centering
		\includegraphics[width=0.7\textwidth]{images/basics/LaplacepyramideProzess.png}
	\caption{Vorgehen Konstruktion einer Laplace-Pyramide}
	\label{fig:Vorgehen Konstruktion einer Laplace-Pyramide}
\end{figure}

\section{Convolutional Neural Networks}
Convolutional Neural Networks, auch bekannt als CNN's, sind ein spezialisierter Typus von neuronalen Netzen, um Daten zu verarbeiten, welche eine gitterartige Topologie aufweisen. Ein Beispiel hierfür sind Bilder, welche eine zwei dimensionale gitterartige Struktur aus Pixeln sind. Der Unterschied zu einfachen neuronalen Netzen ist, dass die CNN's, anstatt einfacher Matrizenmultiplikationen, Faltungsoperation durchführen. In praktischen Anwendungen, wie dem maschinellen Sehen, wurden CNN's immer wieder sehr erfolgreich eingesetzt. 

\subsection{2D-Convolutional Neural Networks}
Für viele praktische Anwendungen sind zwei dimensionale CNN's die erste Wahl. Ob als Autoencoder, zur Objekterkennung in Bildern bis hin zur synthetischen Generierung von Bildern. Die 2D-CNN's haben, anders als der Name erwarten lässt, einen drei dimensionalen Input. Bei Bildern sind zum Beispiel die Farbkanäle die dritte Dimension. Die Zwei im Namen beziffert die Dimensionen in denen sich der Kernel über das Bild bewegt. 
Die 2D-CNN's sind im Normalfall aus mehreren Convolutional-Schichten und Pooling-Schichten aufgebaut. Wenn die Klassifikation von Bildern das Ziel ist, dann werden meist voll verbundene Schichten, als letzte Schichten vor dem Ausgang des Netzes, eingefügt. Hier sollen die Convolutional-Schichten verschiedene Merkmale im Bild herausfiltern und die voll verbundene Schicht mit Hilfe dieser Merkmale das Bild klassifizieren. Wie viele Convolutional-Schichten und Pooling-Schichten in einem 2D-CNN verwendet werden, ist von Anwendungsfall zu Anwendungsfall unterschiedlich. Die optimale Architektur eines 2D-CNN, für einen bestimmten Anwendungsfall, muss durch empirische Forschung ermittelt werden. Durch die intensive Forschung an CNN's, werden immer wieder neue Architekturen und Abwandlungen entwickelt. Die Entwicklung ist in dem Bereich so schnell, dass state-of-the-art Netzarchitekturen sich im Wochenrythmus oder Monatsrythmus ändern. Aufgrund dieser Volatilität und Vielfältigkeit, ist es schwierig ein Beispiel zu finden, welches möglichst allgemeingültig ein 2D-CNN darstellt. Aufgrunddessen wird in diesem Abschnitt auf ein explizites Beispiel, einer Netzarchitektur, verzichtet. Die in dieser Arbeit verwendeten 2D-CNN Architekturen, werden im dritten Kapitel dargestellt und erklärt.

\subsection{1D-Convolutional Neural Networks}
Die im vorherigen Abschnitt beschriebenen 2D-CNN's sind dafür entworfen, und wurden dahingehend fortlaufend weiterentwickelt, um mit 2D-Daten wie Bildern zu arbeiten. Als Alternative zu den konventionellen 2D-CNN's, wurden die 1D-CNN's entwickelt. Alle CNN's haben gleiche Charakteristiken und folgen dem gleichen Ansatz, unabhängig davon ob es ein 1D-, 2D- oder 3D-CNN ist. In der Abbildung 4 ist der Unterschied zwischen 2D-CNN's und 1D-CNN's, an den Beispielen der Verarbeitung von natürlicher Sprache und des maschinellen Sehens, bei der Faltungsoperation visualisiert. In dem Beispiel für das 1D-CNN ist das Eingangsdatum ein Satz, bestehend aus 9 Wörtern. Jedes Wort wird als Vektor, der Convolutional-Schicht, übergeben. Unabhängig von der Länge des Wortes, wird der Filter immer das ganze Worte abdecken. Wie viele Wörter in einem Trainingsschritt betrachtet werden, wird durch die Höhe des Filters festgelegt. In dem Beispiel ist die Höhe des Filters zwei. Folgend wird der Filter, in acht Schritten und einer Dimension, über die Daten geschoben. In dem Beispiel für ein 2D-CNN wird ein Farbbild als Eingangsdatum verarbeitet. Der Filter hat die Höhe und Breite von zwei. Im Gegensatz zum 1D-CNN, wird hier der Filter in zwei Dimensionen, dass heißt horizontal und vertikal, über die Daten geschoben. 

\begin{figure}
	\centering
		\includegraphics[width=0.95\textwidth]{images/basics/1DCNNVS2DCNN.PNG}
	\caption{Vergleich der Faltungsoperation bei 2D-CNN's und 1D-CNN's by Nils Ackermann is licensed under Creative Commons CC BY-ND 4.0}
	\label{fig:Vergleich der Faltungsoperation bei 2D-CNN's und 1D-CNN's}
\end{figure}

Eingangsdaten, für ein 1D-CNN, könnten beispielsweise Zeitfolgen von mehreren Sensoren sein. Diese Zeitfolgen, welche für sich genommen eindimensional sind, könnten auch als Matrix in einem 2D-CNN verarbeitet werden. Bei Untersuchungen zeigte sich jedoch, dass die 1D-CNN's für bestimmte Anwendungfälle zu bervorzugen sind, wenn eindimensionale Daten verarbeitet werden. 

(todo Zitat [47] S. Kiranyaz, T. Ince, R. Hamila, M. Gabbouj, Convolutional Neural Networks for patient-specific ECG classification, in:
Proc. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. EMBS, 2015. doi:10.1109/EMBC.2015.7318926.
[48] S. Kiranyaz, T. Ince, M. Gabbouj, Real-Time Patient-Specific ECG Classification by 1-D Convolutional Neural
Networks, IEEE Trans. Biomed. Eng. 63 (2016) 664?675. doi:10.1109/TBME.2015.2468589.
[49] S. Kiranyaz, T. Ince, M. Gabbouj, Personalized Monitoring and Advance Warning System for Cardiac Arrhythmias, Sci.
Rep. 7 (2017).

Die Gründe hierfür sind:

\begin{itemize}

\item \textbf{In einem 1D-CNN werden, statt Matrixoperationen für die Vor- und Rückpropagation, nur einfach Arrayoperationen durchgeführt. Daraus folgt, dass die Berechnungskomplexität eines 1D-CNN signifikant geringer ist als die eines 2D-CNN.}\footnote[5]{https://moveit.ros.org/}

\item \textbf{Die aktuelle Forschung zeigt, dass 1D-CNN's, auch mit relativ flachen Architekturen, in der Lage sind komplexe Aufgaben zu erlernen, bei denen eindimensionale Daten verarbeitet werden. Im Gegensatz dazu, benötigen 2D-CNN's gewöhnlich tiefere Architekturen, um ähnlich komplexe Aufgaben zu erlernen. Dies hat den Vorteil, dass flachere Architekuren einfacher zu trainieren und zu implementieren sind.#}\footnote[6]{https://ros-planning.github.io/moveit\_tutorials/}

\item \textbf{Aufgrund Ihrer eher geringen Berechnungskomplexität, sind 1D-CNN's gut geeignet für Echtzeitanwendungen und für den energiesparenden Einsatz auf mobilen Geräten. }\footnote[7]{https://github.com/ros-planning}

\end{itemize}


In aktuellen Untersuchungen zeigten 1D-CNN's gute Ergebnisse, besonders bei Anwendungsfällen bei denen die verfügbaren Daten stark begrenzt waren und die Signale aus verschiedenen Quellen hohe Schwankungen zeigten.  Wie in Abbildung3 dargestellt, besteht ein 1D-CNN meist aus Convolutional-Schichten und aus voll vebundenen Schichten zur Klassifikation. Die folgenden Hyperparamater bilden die Konfiguration eines 1D-CNN:

\begin{enumerate} 
	\item Anzahl der Convolutional-Schichten und voll vebundenen Schichten 
	\item Kernelgröße in jeder Convolutional-Schicht
	\item Unterabtastungsfaktor in jeder Convolutional-Schicht
	\item Die Auswahl von Pooling und Aktivierungsfunktionen
\end{enumerate}

\begin{figure}
	\centering
		\includegraphics[width=0.7\textwidth]{images/basics/1DCNN.PNG}
	\caption{Beispiel für eine 1D-CNN-Architektur}
	\label{fig:Beispiel für eine 1D-CNN-Architektur}
\end{figure}



\subsection{3D-Convolutional Neural Networks}
Um in Videos Objekte zu erkennen, werden 2D-CNN's erfolgreich eingesetzt. Bei diesem Anwendungsfall wird jedes Einzelbild, in einem Video, separat verarbeitet. Hierbei werden aber die Änderungen zwischen den Einzelbildern nicht betrachtet und es gehen somit die Information in der zeitlichen Dimension komplett verloren. Diese Information, in der zeitlichen Dimension, können jedoch, für manche Anwendungsfälle, von Bedeutung sein. Um eine weitere Dimension in die Verarbeitung der Daten mit einzubeziehen, wurden die 3D-CNN's entwickelt. Diese weitere Dimension kann die Zeit sein oder auch die dritte räumliche Dimension, um z.B. Volumenbilder zu verarbeiten. In der Abbildung 5 ist der Unterschied zwischen 2D-CNN's und 3D-CNN's dargestellt. Der Kernel wird, bei dem 3D-CNN, in drei Dimensionen über die Daten Bewegt. Für das Beispiel der Videoklassifikation legt die Kerneltiefe D fest, wie viele Bilder in einer Faltung betrachtet werden. Die Breite W und Höhe H des Kernels, haben hier die gleiche Bedeutung wie bei 2D-CNN's. Das Ergebnis der Faltung ist eine 3D-Featuremap. In der Praxis werden die 3D-CNN's noch eher selten verwendet, da durch die Einführung der weiteren Dimension der Speicherbedarf und die Berechnungskomplexität enorm ansteigt. Hinzu kommt, dass die Verfügbarkeit von geeigneten Trainingsdatensätzen noch eingeschränkt ist. Die folgenden Hyperparamater bilden die Konfiguration eines 1D-CNN:

\begin{enumerate} 
	\item Anzahl der Convolutional-Schichten, Pooling-Schichten und voll vebundenen Schichten 
	\item Die Kernelfläche und die Kerneltiefe
	\item Die Auswahl von Regularisierungsfunktionen
	\item Die Auswahl von Aktivierungsfunktionen
\end{enumerate}

\begin{figure}
	\centering
		\includegraphics[width=0.55\textwidth]{images/basics/2DCNNVS3DCNN.png}
	\caption{Vergleich von 2D-CNN und 3D-CNN}
	\label{fig:Vergleich von 2D-CNN und 3D-CNN}
\end{figure}







