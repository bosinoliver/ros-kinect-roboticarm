%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Vorlage für Abschlussarbeiten                                     %%
%%-------------------------------------------------------------------%%
%% Datei:        basics.tex                                         %%
%% Beschreibung: Grundlagenteil welcher verwendete Hard-   %%
%%               und Software näher beschreibt.                            %%
%% Autor: 			 Stefan Herrmann                                     %%
%% Datum:        04.12.2012                                          %%
%% Version:      1.0.1                                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Grundlagen}
\index{Grundlagen} %% Eintrag im Stichwortverzeichnis
In diesem Kapitel wird auf die Grundlagen zu verwendeter Hard- und Software eingegangen. Dies soll Hintergrundinformationen zur gesamten Arbeit und speziell für die Ausführungen im vierten Kapitel bereitstellen. Hierbei wird sich nur auf die wesentlichen Komponenten, für das Gesamtsystem, beschränkt.

\section{Stereokamera}
\index{Stereokamera}
Eine Stereokamera ist ein Kamerasystem,  dass zur Gewinnung von Tiefenbildern genutzt wird. Diese Tiefenbilder enthalten, im Gegensatz zu normalen Farbbildern, den Abstand einzelner Punkte zum Sensor der Kamera. Diese Kamerasysteme haben immer zwei optische Sensoren zur Bilderzeugung. Es gibt unter anderem Systeme mit zwei RGB-Sensoren sowie Systeme mit einem RGB-Sensor und einem Infrarotsensor.

\subsection{Typen}
Unter den Stereokameras gibt es verschiedene Typen, die sich in zwei Gruppen einteilen lassen: Diese Gruppen sind, Kameras mit passiven Verfahren und Kameras mit aktiven Verfahren. Für diese Arbeit wurden die Beschreibungen auf die gängigsten Typen, von Stereokameras, begrenzt.

\subsection*{Embedded Stereo}
\index{Embedded Stereo}
Die ''Embedded Stereo'' Kameras nutzen das passive Stereoverfahren, um Tiefenbilder zu erzeugen. Diese Kamerasysteme haben meist zwei RGB-Sensoren um Bilder aufzunehmen, sowie einen eingebetteten Mikroprozessor oder FPGA, für die Berechnung von Tiefenbildern, integriert. Das passive Stereoverfahren ist an das menschliche Sehen angelehnt, bei dem aus zwei 2D-Bildern ein 3D-Bild gemacht wird. Das passive Verfahren, zur Erzeugung der Tiefenbilder, lässt sich in mehrere Schritte aufteilen. Zu erst wird mit beiden RGB-Sensoren gleichzeitig ein Bild aufgenommen. Anschließend wird an beiden Bildern eine Merkmalextraktion durchgeführt. Diese Bildmerkmale, auch als ''Keypoints'' bezeichnet, lassen Unterschiede zu Ihrer Umgebung erkennen. Die gefundenen Bildmerkmale, beider Bilder, werden bei der Korrespondenzsuche miteinander verglichen. Nachdem die Korrespondenzsuche abgeschlossen ist, ist es noch notwendig falsche Korrespondenzen aus den Ergebnissen herauszufiltern. Mit den endgültigen korrespondierenden Bildmerkmalen kann nun, mittels Triangulation, die Entfernung zu diesem Merkmal im Bild errechnet werden.\cite{SCHMIEDECKE2009,MODROW2008}\\ Um den Prozessor des nutzenden Hauptsystems zu entlasten, werden die Berechnungen, für das beschriebene Verfahren, auf dem eingebetteten Mikroprozessor bzw. FPGA durchgeführt. Da der hohe Rechenaufwand auf eine erhöhte Leistungsaufnahme schließen lässt, werden die ''Embedded Stereo'' Kameras nicht für mobile Systeme empfohlen. 

\subsection*{Time-Of-Flight}
\index{Time-Of-Flight}
Eine''Time-Of-Flight'' Kamera gehört zu den Kamerasystemen die aktive Verfahren, zur Generierung von Tiefenbildern, nutzen. Das von ''Time-Of-Flight'' Kameras genutzte Verfahren ist das Laufzeitverfahren. Bei dem Laufzeitverfahren wird ein Signal von der Kamera ausgesendet und die Zeit ermittelt wie lange das Reflektierte Signal gebraucht hat, um wieder an der Kamera aufzutreffen. Aus der Laufzeit t und Geschwindigkeit v des Signals kann, über die Formel $S =  v t$, direkt auf die Strecke S zum reflektierenden Objekt geschlossen werden. Als typische Emitter-Sensor Systeme, in dem Laufzeitverfahren, zählen Radar-, Ultraschall- und Infrarotsysteme.\cite{SCHMIEDECKE2009}\\ Die ''Time-Of-Flight'' Kameras nutzen in der Regel Infrarotsysteme. Die gängigen ''Time-Of-Flight'' Kameras haben meist einen RGB-Sensor, einen oder mehrere Infrarotemitter und einen Infrarotsensor verbaut. Aufgrund der hohen Ausbreitungsgeschwindigkeit von Licht, werden hohe Anforderungen an die Elektronik, zur Zeitmessung, gestellt. Unter anderem legt die kleinste messbare Zeitdifferenz $\triangle$t die maximale Tiefenauflösung $\triangle$R, mit der Formel $\triangle R = \frac{v \triangle t}{2}$ , fest.\cite{JIANGBUNKE1997} Durch diese hohen Anforderungen an die Elektronik, sind hochauflösende ''Time-Of-Flight'' Kameras kostenintensiv. Da hier Licht, im Infrarotbereich, detektiert wird, sind ''Time-Of-Flight'' Kameras nicht für Bereiche mit direkter Sonneneinstrahlung geeignet.\cite{BLANK2013} In Bereichen mit direkter Sonneneinstrahlung empfehlen sich stattdessen Kameras die auf passive Verfahren zur Tiefengewinnung zurückgreifen.

\subsection*{Infrarotmuster}
\index{Infrarotmuster}
Wie die ''Time-Of-Flight'' Kameras nutzen auch ''Infrarotmuster'' Kameras ein aktives Verfahren, zur Gewinnung von Tiefenbildern. Das Triangulationsverfahren, mit dem Ansatz des codierten Lichts, wird von den ''Infrarotmuster'' Kameras verwendet. Bei diesem Ansatz wird ein Infrarotmuster, mit bekanntem Code, in den Raum projiziert und mit einer Infrarotkamera aufgenommen. Somit sind, über den bekannten Code, alle vorher definierten Messpunkte im Bild eindeutig identifizierbar. Aufgrund der Kenntnis über die Position von Infrarotemitter und Infrarotkamera, kann für jeden Messpunkt der Abstand zur Kamera, über die Triangulation, errechnet werden.\cite{JIANGBUNKE1997,MODROW2008} Wie die ''Time-Of-Flight'' Kameras sind auch die ''Infrarotmuster'' Kameras, aufgrund der Verwendung von Infrarotlicht, nicht für Bereiche mit direkter Sonneneinstrahlung geeignet. Die für die Umsetzung des Gestenerkennungssystems dieser Arbeit genutzte Stereokamera, die ''Kinect'', ist eine ''Infrarotmuster'' Kamera. Somit ist das, in dieser Bachelorarbeit, entwickelte Gestenerkennungssystem nur für Innenbereiche geeignet.

\section{OpenNI}
\index{OpenNI}
Das Framework ''OpenNI'' soll in diesem Abschnitt, in seiner Funktion und seinem Aufbau, beschrieben werden. 

\subsection{Was ist OpenNI}
''OpenNI'' ist ein Framework, dass verschiedene Programmiersprachen erlaubt und für verschiedene Plattformen verfügbar ist. Weiterhin definiert es API's zur Entwicklung von Anwendungen, die natürliche Interaktion, wie Sprache und Gesten, verwenden. Die API's von OpenNI bestehen aus mehreren Schnittstellen, zur Entwicklung von NI-Anwendungen. Der Hauptzweck von ''OpenNI'' ist es, eine Standard-API zur Verfügung zu stellen, die eine Kommunikation mit folgenden Komponenten ermöglicht:

\begin{itemize}
\item optischen und akustischen Sensoren
\item Middleware, die Funktionen zur Verarbeitung von optischen und akustischen Sensordaten implementiert
\end{itemize}

''OpenNI'' liefert hierzu Schnittstellen die von den Sensorgeräten implementiert werden müssen sowie Schnittstellen die von der Middleware implementiert werden müssen. Durch diesen Ansatz werden Abhängigkeiten, zwischen Sensorgeräten und Middleware, vermieden. Dies ermöglicht es Anwendungen, ohne den Aufwand der Portierung, auf der Basis von verschiedenen Middleware's zu arbeiten. Des Weiteren bietet ''OpenNI'' die Möglichkeit, Anwendungen zu entwickeln die Sensorrohdaten verwenden, unabhängig vom Sensor der diese liefert.\cite{OPENNI}

\subsection{Module}
\index{Module}
Als Module werden Gerätetreiber und Middleware bezeichnet, die im ''OpenNI'' Framework registriert wurden. Die folgenden Module werden von ''OpenNI'' unterstützt:\cite{OPENNI}

\subsubsection*{Sensormodule}
\begin{itemize}
\item 3D Sensor
\item RGB-Kamera
\item Infrarotkamera
\item Mikrofon
\end{itemize}

\subsubsection*{Middleware-Module}
\begin{itemize}
\item Middleware zur Ganzkörperanalyse
\item Middleware zur Analyse der Handposition
\item Middleware zur Gestenerkennung
\item Middleware zur Analyse von Szenen in Bildern
\end{itemize}

\subsection{Kompatibilität und Verfügbarkeit}
Die Entwickler von ''OpenNI'' garantieren volle Rückwärtskompatibilität\cite{OPENNI}. Dies bedeutet das Anwendungen, die auf Basis irgendeiner Version von ''OpenNI'' entwickelt wurden, mit neueren Versionen von ''OpenNI'' arbeiten können.\\
''OpenNI'' ist für folgende Betriebssysteme verfügbar\cite{OPENNI}:

\begin{itemize}
\item Windows XP und aktueller
\item Linux Ubuntu 10.10 und aktueller
\end{itemize}

\section{ROS}
\index{ROS}
Dieser Abschnitt soll eine Einführung in das Konzept, die Komponenten und das Vokabular von ROS geben. Diese Informationen sind, zum Verständnis des vierten Kapitels, elementar. 
Die folgenden Punkte werden in dieser Einführung behandelt:

\begin{itemize}
\item Was ist ROS
\item Konzept und Komponenten
\item Wichtige ROS-Befehle
\end{itemize}

\subsection{Was ist ROS}
Da ROS eine Vielzahl von Aufgaben eines Betriebssystems übernimmt, aber als Umgebung ein Linux Betriebssystem benötigt, wird es oft als ''Meta-Betriebssystem'' bezeichnet. Die elementare Aufgabe von ROS ist es, eine Kommunikation zwischen Nutzer, Betriebssystem und externer Hardware bereitzustellen. Zur externen Hardware, mit der kommuniziert werden kann, zählen beispielsweise Sensoren, Kameras und Roboter. In dem ROS sich wie ein Betriebssystem verhält, bringt es den Vorteil der Hardwareabstraktion. Somit kann ein Nutzer einen Roboter steuern, ohne große Kenntnis über dessen Hardware- und Softwaredetails zu haben. Als quelloffenes Framework, ist ROS für jeden frei verfügbar. Dieser Ansatz wird auch bei den meisten ROS-Paketen verfolgt und diese unter der BSD-3-Lizenz veröffentlicht. Aufgrund dieser Voraussetzungen, werden neue Entwicklungen und neues Wissen, in der ROS-Community, schnell verbreitet. Aber auch für Unternehmen ist ROS interessant, da auch kommerzielle Produkte mit ROS entwickelt werden können. Hierfür ist die einzige Bedingung, dass der Copyright-Vermerk der ursprünglichen Software zitiert wird. \cite{FAIRCHILD2016,ROS}

\subsection{Konzept und Komponenten}
Das Konzept von ROS teilt sich in drei Konzeptschichten auf\cite{ROS}:

\begin{itemize}
\item ROS Dateisystem
\item ROS Computation Graph
\item ROS Community
\end{itemize}

\subsubsection*{ROS Dateisystem}
Das ROS-Dateisystem setzt sich aus folgenden Komponenten zusammen:

\begin{itemize}
\item Packages: In Paketen wird die Software in ROS organisiert.
\item Manifests: Im Manifest sind Metadaten über das Paket enthalten. Diese Manifests sind in XML geschrieben.
\item Stacks: Pakete, die gemeinsam eine Funktion bereitstellen, werden in Stacks zusammengefasst. 
\item Stack Manifest: Im Stack Manifest sind Metadaten über das Stack enthalten.\cite{FAIRCHILD2016,ROS}
\end{itemize}

\subsubsection*{ROS Computation Graph}
Das Netzwerk von Prozessen, die eine gemeinsame Aufgabe erfüllen, in ROS bildet den ''Computation Graph''. Die folgenden Komponenten versorgen den Graph mit Daten:

\begin{itemize}
\item Nodes: Als Knoten werden Prozesse bezeichnet, welche eine Aktion ausführen. Diese Knoten haben die Möglichkeit sich bei dem ROS Master zu registrieren und untereinander zu kommunizieren. Die Verbindungsinformationen, zur Kommunikation untereinander, erhalten die Knoten vom ROS Master. Knoten können auf verschiedene Arten gestartet werden. Zum einen über ein Terminalfenster durch ausgeführte Befehle, zum anderen als Teil eines Programms. 
\item Master: 
\item Parameter Server: 
\item Messages: 
\item Topics: 
\item Services: 
\item Bags: 
\end{itemize}

\subsubsection*{ROS Community}


\subsection{Wichtige ROS-Befehle}

\section{MoveIt!}
\index{MoveIt!}